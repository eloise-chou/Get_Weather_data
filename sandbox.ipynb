{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All weather station data from 2015 - 2023/09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://farmer.iyard.org/date/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the <a> tags on the page\n",
    "    all_a_tags = soup.find_all('a')\n",
    "\n",
    "    # Extract content inside each <a> tag ending with \".zip\"\n",
    "    zip_links = [a_tag['href'] for a_tag in all_a_tags if a_tag.get('href', '').endswith('.zip')]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(zip_links[:-1], columns=['zip_link'])\n",
    "\n",
    "    base_url = \"http://farmer.iyard.org\"\n",
    "    df['full_link'] = base_url + df['zip_link']\n",
    "\n",
    "    df.to_csv('./DATA/file_links/weather_links.csv', index = False)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_link = pd.read_csv('./DATA/file_links/weather_links.csv')\n",
    "\n",
    "for index, row in weather_link.iterrows():\n",
    "    link = row[\"full_link\"]\n",
    "    response = requests.get(link, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the filename from the URL\n",
    "        filename = link.split(\"/\")[-1]\n",
    "\n",
    "        # Save the content to the file\n",
    "        with open('./DATA/all_raw_data/' + filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file from {link}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taipei weather station data from 2015 - 2023/09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei_weather_station = pd.read_csv('./DATA/taipei_weather_station.csv')\n",
    "\n",
    "taipei_weather_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://farmer.iyard.org/station/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the <a> tags on the page\n",
    "    filter_values = taipei_weather_station['代號'].to_list()\n",
    "\n",
    "    # Extract content inside each <a> tag ending with \".zip\" and containing the filter values\n",
    "    zip_links = [a_tag['href'] for a_tag in all_a_tags\n",
    "                 if a_tag.get('href', '').endswith('.zip') and any(value in a_tag['href'] for value in filter_values)]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(zip_links, columns=['zip_link'])\n",
    "\n",
    "    base_url = \"http://farmer.iyard.org\"\n",
    "    df['full_link'] = base_url + df['zip_link']\n",
    "\n",
    "    df.to_csv('./DATA/file_links/taipei_weather_links.csv', index = False)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei_weather_link = pd.read_csv('./DATA/file_links/taipei_weather_links.csv')\n",
    "\n",
    "for index, row in taipei_weather_link.iterrows():\n",
    "    link = row[\"full_link\"]\n",
    "    response = requests.get(link, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the filename from the URL\n",
    "        filename = link.split(\"/\")[-1]\n",
    "\n",
    "        # Save the content to the file\n",
    "        with open('./DATA/taipei_raw_data/' + filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file from {link}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all bureau data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the list of folders\n",
    "bureau_folders = ['466920', '466930', '466910', 'CAAH60', '466960', 'A0A460']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "bureau_concatenated_data = pd.DataFrame(columns = list(range(1, 31)))\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder_name in bureau_folders:\n",
    "    # Construct the path to the zip file\n",
    "    zip_path = f\"./DATA/taipei_raw_data/{folder_name}.zip\"\n",
    "\n",
    "    \n",
    "    # Open the zipped dataset\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        # Find all files in the zip file\n",
    "        files_in_zip = z.namelist()\n",
    "\n",
    "        # Iterate through each file in the zip file\n",
    "        for file_in_zip in files_in_zip:\n",
    "            # Open the csv file in the dataset\n",
    "            with z.open(file_in_zip) as f:\n",
    "                # Read the dataset\n",
    "                data = pd.read_csv(f, sep=',', encoding='big5', names = list(range(1, 31)))\n",
    "\n",
    "                # Assuming your date column is at index 5\n",
    "                data[5] = pd.to_datetime(data[5])\n",
    "                data[5] = data[5].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "                # Concatenate the data to the main DataFrame\n",
    "                bureau_concatenated_data = pd.concat([bureau_concatenated_data, data], ignore_index=True)\n",
    "                print(file_in_zip)\n",
    "\n",
    "# Save the concatenated data to a new file or perform further analysis\n",
    "bureau_concatenated_data.to_csv('./DATA/bureau_station_all_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_column_version_1 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', '?', 'ELEV', 'WDIR', 'WDSD', 'TEMP',\n",
    "                           'HUMD', 'PRES', '24R', 'H_FX', 'H_XD', 'H_FXT', 'H_F10', 'H_10D', 'H_F10T', 'CITY', 'CITY_SN',\n",
    "                           'TOWN', 'TOWN_SN']\n",
    "\n",
    "bureau_column_version_2 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'ELEV', 'WDIR', 'WDSD', 'TEMP', 'HUMD', \n",
    "                           'PRES', '24R', 'H_FX', 'H_XD', 'H_FXT', 'H_F10', 'H_10D', 'H_F10T', 'H_UVI', 'CITY', 'CITY_SN',\n",
    "                           'TOWN', 'TOWN_SN']\n",
    "\n",
    "bureau_column_version_3 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'ELEV', 'WDIR', 'WDSD', 'TEMP', 'HUMD', \n",
    "                           'PRES', '24R', 'H_FX', 'H_XD', 'H_FXT', 'H_F10', 'H_10D', 'H_F10T', 'H_UVI', 'D_TX', 'D_TXT',\n",
    "                           'D_TN', 'D_TNT', 'D_TS', 'H_VIS', 'H_Weather', 'CITY', 'CITY_SN', 'TOWN', 'TOWN_SN']\n",
    "\n",
    "col_wanted = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'WDSD', 'TEMP', 'HUMD',\n",
    "              '24R', 'TOWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part1：2017-6-21 15:10之前\n",
    "bureau_weather_part1 = bureau_concatenated_data[bureau_concatenated_data[5] < '2017-06-21 15:20']\n",
    "\n",
    "bureau_weather_part1.dropna(how='all', axis=1, inplace=True)\n",
    "bureau_weather_part1.columns = bureau_column_version_1\n",
    "bureau_weather_part1_wanted_columns = bureau_weather_part1[col_wanted]\n",
    "\n",
    "bureau_weather_part1_wanted_columns.to_csv('./DATA/organized_data/bureau_weather_part1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part2：2019-10-21 10:10之前\n",
    "bureau_weather_part2 = (\n",
    "    bureau_concatenated_data[(bureau_concatenated_data[5] > '2017-06-21 15:10') & \n",
    "                             (bureau_concatenated_data[5] < '2019-10-21 10:20')]\n",
    "    )\n",
    "\n",
    "bureau_weather_part2.dropna(how='all', axis=1, inplace=True)\n",
    "bureau_weather_part2.columns = bureau_column_version_2\n",
    "bureau_weather_part2_wanted_columns = bureau_weather_part2[col_wanted]\n",
    "\n",
    "bureau_weather_part2_wanted_columns\n",
    "\n",
    "bureau_weather_part2_wanted_columns.to_csv('./DATA/organized_data/bureau_weather_part2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3：2019-10-21 10:20之後\n",
    "bureau_weather_part3 = bureau_concatenated_data[bureau_concatenated_data[5] > '2019-10-21 10:10']\n",
    "\n",
    "bureau_weather_part3.dropna(how='all', axis=1, inplace=True)\n",
    "bureau_weather_part3.columns = bureau_column_version_3\n",
    "bureau_weather_part3_wanted_columns = bureau_weather_part3[col_wanted]\n",
    "\n",
    "bureau_weather_part3_wanted_columns.to_csv('./DATA/organized_data/bureau_weather_part3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all auto station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the list of folders\n",
    "auto_folders = ['C0A9E0','C0A9C0','CM0020','C0A980','C0AH40','C0A770','C0A9A0','C0A9F0',\n",
    "                  'C0AC80','C0AI40','C0A9B0','C0AC40','C0AC70','C0A9G0','C0AH70','CAA040','CAA090']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "auto_concatenated_data = pd.DataFrame(columns = list(range(1, 31)))\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder_name in auto_folders:\n",
    "    # Construct the path to the zip file\n",
    "    zip_path = f\"./DATA/taipei_raw_data/{folder_name}.zip\"\n",
    "\n",
    "    \n",
    "    # Open the zipped dataset\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        # Find all files in the zip file\n",
    "        files_in_zip = z.namelist()\n",
    "\n",
    "        # Iterate through each file in the zip file\n",
    "        for file_in_zip in files_in_zip:\n",
    "            # Open the csv file in the dataset\n",
    "            with z.open(file_in_zip) as f:\n",
    "                # Read the dataset\n",
    "                data = pd.read_csv(f, sep=',', encoding='big5', names = list(range(1, 31)))\n",
    "\n",
    "                # Assuming your date column is at index 5\n",
    "                data[5] = pd.to_datetime(data[5])\n",
    "                data[5] = data[5].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "                # Concatenate the data to the main DataFrame\n",
    "                auto_concatenated_data = pd.concat([auto_concatenated_data, data], ignore_index=True)\n",
    "                print(file_in_zip)\n",
    "\n",
    "# Save the concatenated data to a new file or perform further analysis\n",
    "auto_concatenated_data.to_csv('./DATA/auto_station_all_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_column_version_1 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'ELEV', 'WDIR', 'WDSD', 'TEMP',\n",
    "                         'HUMD', 'PRES', 'SUN', '24R','WS15M','WD15M','WS15T', 'CITY', 'CITY_SN',\n",
    "                         'TOWN', 'TOWN_SN']\n",
    "\n",
    "auto_column_version_2 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'ELEV', 'WDIR', 'WDSD', 'TEMP',\n",
    "                         'HUMD', 'PRES', '24R', 'H_FX', 'H_XD', 'H_FXT', 'D_TX', 'CITY', 'CITY_SN',\n",
    "                         'TOWN', 'TOWN_SN']\n",
    "\n",
    "auto_column_version_3 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'ELEV', 'WDIR', 'WDSD', 'TEMP',\n",
    "                         'HUMD', 'PRES', '24R', 'H_FX', 'H_XD', 'H_FXT', 'D_TX', 'D_TXT', 'D_TN', 'D_TNT', \n",
    "                         'CITY', 'CITY_SN', 'TOWN', 'TOWN_SN']\n",
    "\n",
    "# only for CAA040 & CAA090\n",
    "auto_column_version_4 = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'ELEV', 'WDIR', 'WDSD', 'TEMP', 'HUMD', \n",
    "                         'PRES', '24R', 'H_FX', 'H_XD', 'H_FXT', 'H_F10', 'H_10D', 'H_F10T', 'H_UVI', 'D_TX', 'D_TXT',\n",
    "                         'D_TN', 'D_TNT', 'D_TS', 'H_VIS', 'H_Weather', 'CITY', 'CITY_SN', 'TOWN', 'TOWN_SN']\n",
    "\n",
    "col_wanted = ['lat', 'lon', 'locationName', 'stationId', 'obsTime', 'WDSD', 'TEMP', 'HUMD',\n",
    "              '24R', 'TOWN']\n",
    "\n",
    "exclude_weather_station = ['CAA040','CAA090']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part1：2019-9-19 11:00之前 and is not 'CAA040','CAA090'\n",
    "auto_weather_part1 = auto_concatenated_data[\n",
    "    (auto_concatenated_data[5] < '2019-09-19 11:10') & \n",
    "    (~auto_concatenated_data[4].isin(exclude_weather_station))]\n",
    "\n",
    "auto_weather_part1.dropna(how='all', axis=1, inplace=True)\n",
    "auto_weather_part1.columns = auto_column_version_1\n",
    "auto_weather_part1_wanted_columns = auto_weather_part1[col_wanted]\n",
    "\n",
    "auto_weather_part1_wanted_columns.to_csv('./DATA/organized_data/auto_weather_part1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part2：2019-9-19 11:00之後 and 2019-10-21 10:00之前 and is not 'CAA040','CAA090'\n",
    "auto_weather_part2 = auto_concatenated_data[\n",
    "    (auto_concatenated_data[5] > '2019-09-19 11:00') & \n",
    "    (auto_concatenated_data[5] < '2019-10-21 10:10') &\n",
    "    (~auto_concatenated_data[4].isin(exclude_weather_station))]\n",
    "\n",
    "auto_weather_part2.dropna(how='all', axis=1, inplace=True)\n",
    "auto_weather_part2.columns = auto_column_version_2\n",
    "auto_weather_part2_wanted_columns = auto_weather_part2[col_wanted]\n",
    "\n",
    "auto_weather_part2_wanted_columns.to_csv('./DATA/organized_data/auto_weather_part2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3：2019-10-21 11:00之後 and is not 'CAA040','CAA090'\n",
    "auto_weather_part3 = auto_concatenated_data[\n",
    "    (auto_concatenated_data[5] > '2019-10-21 10:50') & \n",
    "    (~auto_concatenated_data[4].isin(exclude_weather_station))]\n",
    "\n",
    "auto_weather_part3.dropna(how='all', axis=1, inplace=True)\n",
    "auto_weather_part3.columns = auto_column_version_3\n",
    "auto_weather_part3_wanted_columns = auto_weather_part3[col_wanted]\n",
    "\n",
    "auto_weather_part3_wanted_columns.to_csv('./DATA/organized_data/auto_weather_part3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part4：'CAA040','CAA090'\n",
    "auto_weather_part4 = auto_concatenated_data[\n",
    "    auto_concatenated_data[4].isin(exclude_weather_station)]\n",
    "\n",
    "auto_weather_part4.dropna(how='all', axis=1, inplace=True)\n",
    "auto_weather_part4.columns = auto_column_version_4\n",
    "auto_weather_part4_wanted_columns = auto_weather_part4[col_wanted]\n",
    "\n",
    "auto_weather_part4_wanted_columns.to_csv('./DATA/organized_data/auto_weather_part4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part5：is'CAA040','CAA090' but only contains 23 columns >> auto_column_version_3\n",
    "auto_weather_part5 = auto_concatenated_data[\n",
    "    auto_concatenated_data[4].isin(exclude_weather_station)]\n",
    "\n",
    "auto_weather_part5 = auto_weather_part5[auto_weather_part5.isnull().any(axis=1)]\n",
    "auto_weather_part5.dropna(how='all', axis=1, inplace=True)\n",
    "auto_weather_part5.columns = auto_column_version_3\n",
    "auto_weather_part5_wanted_columns = auto_weather_part5[col_wanted]\n",
    "\n",
    "auto_weather_part5_wanted_columns.to_csv('./DATA/organized_data/auto_weather_part5.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate bureau & auto station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data(folder_path, file_extension='csv'):\n",
    "    global concatenated_data\n",
    "\n",
    "    # Get a list of all files in the folder with the specified extension\n",
    "    file_pattern = f\"{folder_path}/*.{file_extension}\"\n",
    "    file_list = glob(file_pattern)\n",
    "\n",
    "    concatenated_organized_data = pd.DataFrame(columns = col_wanted )\n",
    "\n",
    "    # Loop through each file and concatenate its data to the global DataFrame\n",
    "    for file_path in file_list:\n",
    "        current_data = pd.read_csv(file_path)  # Assuming the files are in CSV format, adjust accordingly\n",
    "        concatenated_organized_data = pd.concat([concatenated_organized_data, current_data], ignore_index=True)\n",
    "\n",
    "    return concatenated_organized_data\n",
    "\n",
    "# Call the function with the folder path\n",
    "\n",
    "\n",
    "# Now, concatenated_data contains all the data from the files in the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eloise.chou\\AppData\\Local\\Temp\\ipykernel_35324\\2264275203.py:12: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  current_data = pd.read_csv(file_path)  # Assuming the files are in CSV format, adjust accordingly\n",
      "C:\\Users\\eloise.chou\\AppData\\Local\\Temp\\ipykernel_35324\\2264275203.py:12: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  current_data = pd.read_csv(file_path)  # Assuming the files are in CSV format, adjust accordingly\n"
     ]
    }
   ],
   "source": [
    "folder_path = './DATA/organized_data'\n",
    "\n",
    "concatenated_organized_data = concatenate_data(folder_path)\n",
    "\n",
    "sorted_data = concatenated_organized_data.sort_values(by=['stationId', 'obsTime'])\n",
    "\n",
    "sorted_data.to_csv('./DATA/all_weather_data_sort_by_station_time.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
